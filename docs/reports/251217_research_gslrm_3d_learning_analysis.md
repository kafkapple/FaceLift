---
date: 2024-12-17
context_name: "2_Research"
tags: [ai-assisted, mouse-facelift, gs-lrm, objaverse, 3d-learning, synthetic-data]
project: mouse-facelift
status: completed
generator: ai-assisted
generator_tool: claude-code
---

# GS-LRM 3D 학습 심층 분석: Objaverse 성공 vs Mouse 실패 원인

## 1. 핵심 질문

> **"왜 Objaverse는 진정한 3D 학습이 가능했고, Mouse 데이터는 안 되는가?"**

---

## 2. Objaverse 3D 학습 성공 원인 분석

### 2.1 데이터의 근본적 차이: 렌더링 vs 촬영

```
┌─────────────────────────────────────────────────────────────────────┐
│                    Objaverse 데이터 생성 과정                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│   [3D 모델]  ──렌더링──>  [Multi-view 이미지]                         │
│      │                         │                                     │
│      │                         │                                     │
│      ▼                         ▼                                     │
│   완벽한 3D          모든 뷰가 동일한 3D에서                          │
│   기하학 정보         수학적으로 정확하게 생성                         │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                    Mouse 데이터 촬영 과정                            │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│   [실제 마우스]  ──촬영──>  [6개 카메라 이미지]                        │
│      │                         │                                     │
│      │                         │                                     │
│      ▼                         ▼                                     │
│   3D Ground Truth          시간에 따른 마우스 움직임                   │
│   존재하지 않음             조명 변화, 카메라 오차                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 2.2 Objaverse 성공의 핵심 요소

| 요소 | Objaverse | Mouse 데이터 | 영향 |
|------|-----------|--------------|------|
| **3D Ground Truth** | 존재 (원본 3D 모델) | 없음 | 치명적 |
| **Multi-view 일관성** | 완벽 (동일 3D에서 렌더링) | 불완전 (시간 차이, 움직임) | 매우 큼 |
| **카메라 파라미터** | 정확 (렌더링 시 계산) | 캘리브레이션 오차 | 중간 |
| **조명 조건** | 일관됨 | 변화 가능 | 작음 |
| **데이터 양** | ~80,000 객체 | ~1,600 프레임 | 큼 |
| **객체 다양성** | 매우 높음 (다양한 형태) | 낮음 (단일 마우스) | 큼 |

### 2.3 왜 2D Loss로도 Objaverse에서는 3D가 학습되는가?

```python
# Objaverse 학습 시나리오
# 입력: 4개 뷰 (cam_0, cam_4, cam_8, cam_12)
# 타겟: 나머지 뷰들

# 핵심: 모든 뷰가 "동일한 3D 객체"에서 렌더링됨
#       → 2D loss를 최소화하는 유일한 방법 = 올바른 3D 학습

# 수학적으로:
# min_θ Σ_v ||Render(G_θ, cam_v) - GT_v||²
#
# GT_v = Render(True3D, cam_v)  (Objaverse)
# → G_θ가 True3D에 수렴해야만 모든 뷰에서 loss 최소화 가능

# 반면 Mouse 데이터:
# GT_v = RealPhoto(Mouse_at_time_t, cam_v)  (시간 t에서 촬영)
# → 마우스가 시간에 따라 움직이면 3D 일관성 깨짐
# → "평균 이미지"가 loss를 더 잘 최소화할 수 있음
```

**핵심 인사이트**:
> Objaverse에서 2D loss가 3D를 학습시키는 이유는 **모든 GT 이미지가 동일한 3D에서 수학적으로 정확하게 렌더링**되었기 때문. 이 경우 올바른 3D를 학습하는 것이 모든 뷰의 2D loss를 최소화하는 **유일한 방법**임.

---

## 3. 현재 GS-LRM 데이터 파이프라인 분석

### 3.1 데이터셋 구조 (opencv_cameras.json 기반)

```
data_sample/gslrm/sample_000/
├── opencv_cameras.json    # 카메라 파라미터 (32개 뷰)
└── images/
    ├── cam_000.png        # View 0 (azimuth 0°)
    ├── cam_001.png        # View 1 (azimuth 11.25°)
    ├── ...
    └── cam_031.png        # View 31 (azimuth 348.75°)
```

### 3.2 카메라 배치 분석

```python
# opencv_cameras.json 분석 결과:
# - 32개 카메라 (360° / 32 = 11.25° 간격)
# - Elevation: 약 20° (고정)
# - Distance: 2.7 units (고정)
# - FOV: fx=fy=548.99, cx=cy=256 (512x512 이미지)

# 카메라 위치 예시:
cam_000: azimuth=0°,    position=(2.54, 0.00, 0.92)
cam_008: azimuth=90°,   position=(0.00, 2.54, 0.92)
cam_016: azimuth=180°,  position=(-2.54, 0.00, 0.92)
cam_024: azimuth=270°,  position=(0.00, -2.54, 0.92)
```

### 3.3 GS-LRM 입력 데이터 흐름

```
┌─────────────────────────────────────────────────────────────────────┐
│                    RandomViewDataset 데이터 흐름                     │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  1. 샘플 로드                                                        │
│     └── opencv_cameras.json → 32개 카메라 정보                       │
│                                                                      │
│  2. 뷰 선택 (랜덤)                                                   │
│     └── num_views=8 (기본) 중 num_input_views=4 선택                 │
│     └── 입력 4개 + 타겟 4개 (또는 전체 8개가 타겟)                    │
│                                                                      │
│  3. 이미지 처리                                                      │
│     └── 리사이즈 → 512x512                                          │
│     └── 배경 처리 (RGBA → RGB with bg_color)                        │
│     └── 정규화 → [0, 1]                                             │
│                                                                      │
│  4. 출력 형식                                                        │
│     {                                                                │
│       "image": [V, C, H, W],      # V=num_views, C=3/4              │
│       "c2w": [V, 4, 4],           # Camera-to-World 행렬            │
│       "fxfycxcy": [V, 4],         # 카메라 내부 파라미터             │
│       "index": [V, 2],            # (view_idx, scene_idx)           │
│       "bg_color": [3]             # 배경 색상                        │
│     }                                                                │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.4 Mouse 데이터셋 특수 처리 (MouseViewDataset)

```python
# gslrm/data/mouse_dataset.py의 주요 차이점:

class MouseViewDataset(Dataset):
    # 1. 고정 6개 뷰 (랜덤 샘플링 최소화)
    num_views = 6  # vs RandomViewDataset의 8개

    # 2. 단일 입력 뷰 (inference 시)
    num_input_views = 1  # vs 4개

    # 3. 데이터 증강 (제한된 데이터 보완)
    augmentation:
      - horizontal_flip
      - rotation_range
      - brightness_range
      - contrast_range

    # 4. 일관된 증강 (모든 뷰에 동일 적용)
    _apply_consistent_augmentation()
```

---

## 4. 합성 데이터 구축 전략

### 4.1 문제 정의: 6개 실제 뷰로 무엇이 가능한가?

```
┌─────────────────────────────────────────────────────────────────────┐
│                    현재 보유 데이터                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│   6개 실제 촬영 뷰 (60° 간격)                                        │
│   ┌────┐  ┌────┐  ┌────┐  ┌────┐  ┌────┐  ┌────┐                   │
│   │ 0° │  │60° │  │120°│  │180°│  │240°│  │300°│                   │
│   └────┘  └────┘  └────┘  └────┘  └────┘  └────┘                   │
│                                                                      │
│   질문: 이 6개 뷰만으로 32개 뷰 데이터셋 구축 가능?                   │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.2 새 뷰 생성 방법: MVDiffusion 활용

```
┌─────────────────────────────────────────────────────────────────────┐
│                    MVDiffusion 기반 새 뷰 생성                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│   방법 1: 단일 뷰 → 6개 뷰 생성 (현재 파이프라인)                    │
│   ┌────┐                                                            │
│   │입력│ ──MVDiffusion──> [6개 생성 뷰]                             │
│   └────┘                                                            │
│                                                                      │
│   방법 2: 6개 실제 뷰 → 중간 뷰 보간 생성                            │
│   ┌────┐     ┌────┐                                                 │
│   │ 0° │ ─?─ │60° │  → 30° 뷰 생성 가능?                            │
│   └────┘     └────┘                                                 │
│                                                                      │
│   방법 3: Cross-view consistency를 활용한 정제                       │
│   - 실제 뷰를 조건으로 생성 뷰 품질 향상                             │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.3 구체적 합성 데이터 구축 전략

#### 전략 A: MVDiffusion 자기 증류 (Self-Distillation)

```python
# 개념: 각 실제 뷰를 입력으로 MVDiffusion 실행
# 결과: 6 × 6 = 36개 뷰 (중복 포함)

for real_view in [0, 60, 120, 180, 240, 300]:
    input_image = load_image(real_view)
    generated_views = mvdiffusion(input_image)  # 6개 생성

    # 생성된 뷰 중 실제와 겹치는 각도는 실제 이미지로 대체
    # 나머지는 합성 뷰로 사용

# 장점:
# - 추가 모델 없이 기존 MVDiffusion 활용
# - 6개 관점에서 생성 → 다양성 증가

# 단점:
# - 생성 뷰 간 일관성 보장 안 됨
# - 같은 각도에서 6번 생성 → 어떤 것이 정답?
```

#### 전략 B: Objaverse 혼합 학습

```yaml
# configs/mouse_gslrm_mixed.yaml
training:
  dataset:
    # 혼합 데이터셋 사용
    type: "MixedDataset"
    datasets:
      - name: "objaverse"
        path: "/data/objaverse_80k_train.txt"
        weight: 0.8  # 80% Objaverse
      - name: "mouse_real"
        path: "/data/mouse_real_train.txt"
        weight: 0.1  # 10% 실제 마우스
      - name: "mouse_synthetic"
        path: "/data/mouse_synthetic_train.txt"
        weight: 0.1  # 10% 합성 마우스

# 학습 전략:
# 1단계: Objaverse만으로 3D prior 학습 (또는 pretrained 사용)
# 2단계: 혼합 데이터로 domain adaptation
# 3단계: 점진적으로 mouse 비율 증가
```

#### 전략 C: 6뷰 기반 새 뷰 생성 (Novel View Synthesis)

```python
# 방법: 기존 6개 뷰에서 중간 뷰 생성
# 필요 기술: View interpolation / Novel view synthesis

# 옵션 1: Zero123++ 활용
# - 단일 이미지 + 목표 카메라 → 새 뷰 생성
# - 각 실제 뷰에서 ±30° 위치 생성

# 옵션 2: 3D Gaussian Splatting 먼저 수행
# - 6개 뷰로 간이 3DGS 최적화
# - 새로운 각도에서 렌더링
# - 이 렌더링을 합성 데이터로 활용

# 옵션 3: NeRF 기반 보간
# - 6개 뷰로 NeRF 학습
# - 새로운 카메라 위치에서 렌더링
```

### 4.4 권장 구현 전략: 단계별 접근

```
┌─────────────────────────────────────────────────────────────────────┐
│                    단계별 합성 데이터 구축 전략                       │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  Phase 1: 기존 6개 뷰 최대 활용 (즉시 가능)                          │
│  ────────────────────────────────────────────                        │
│  - 각 실제 뷰를 MVDiffusion 입력으로 사용                            │
│  - 6개 입력 × 6개 출력 = 36개 뷰 (중복 제거 후 ~12개 고유 각도)      │
│  - 데이터 증강: flip, rotation, color jitter                        │
│  - 예상 데이터: 1,600 × 12 × augmentation = ~50,000+ 샘플           │
│                                                                      │
│  Phase 2: Objaverse 혼합 학습 (권장)                                 │
│  ────────────────────────────────────────────                        │
│  - Objaverse 80K + Mouse 합성 혼합                                  │
│  - 비율: Objaverse 80% + Mouse 20%                                  │
│  - Objaverse로 3D prior 유지하면서 mouse 도메인 적응                 │
│                                                                      │
│  Phase 3: 고급 Novel View Synthesis (선택적)                         │
│  ────────────────────────────────────────────                        │
│  - 6개 뷰로 간이 3DGS/NeRF 생성                                      │
│  - 중간 각도 (30°, 90°, 150° 등) 렌더링                             │
│  - 이를 추가 학습 데이터로 활용                                      │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 5. 6개 뷰만으로 가능한가?

### 5.1 이론적 분석

```
질문: 6개 뷰(60° 간격)만으로 32개 뷰 데이터셋 구축 가능?

답: 직접적으로는 불가능, 간접적으로는 가능

이유:
1. 직접 불가능
   - 6개 뷰에는 30°, 90°, 150° 등의 정보가 없음
   - 보이지 않는 각도의 GT를 만들 수 없음

2. 간접 가능 (생성 모델 활용)
   - MVDiffusion이 30°, 90° 등의 뷰를 "생성"
   - 생성된 뷰가 GT 역할 (pseudo-GT)
   - 문제: 생성 품질이 학습 품질의 상한
```

### 5.2 실용적 권장사항

| 접근법 | 가능성 | 품질 | 복잡도 | 권장도 |
|--------|--------|------|--------|--------|
| 6개 뷰 그대로 사용 | O | 낮음 | 낮음 | △ |
| MVDiffusion 자기 증류 | O | 중간 | 중간 | O |
| Objaverse 혼합 | O | 높음 | 중간 | ◎ |
| 6뷰 → NeRF → 렌더링 | △ | 중간 | 높음 | △ |
| 3D 스캔 데이터 확보 | O | 매우 높음 | 매우 높음 | (장기) |

---

## 6. 결론 및 권장 사항

### 6.1 핵심 발견

1. **Objaverse 성공 이유**: 렌더링 데이터라서 모든 뷰가 동일한 3D에서 수학적으로 정확하게 생성됨. 이 경우 2D loss만으로도 올바른 3D를 학습하는 것이 유일한 최적해.

2. **Mouse 실패 이유**: 실제 촬영 데이터는 시간 차이, 마우스 움직임, 카메라 오차로 인해 완벽한 3D 일관성이 없음. "평균 이미지"가 loss를 더 잘 최소화할 수 있어 mode collapse 발생.

3. **합성 데이터의 한계**: MVDiffusion으로 생성한 뷰는 pseudo-GT. 생성 품질이 학습의 상한이 됨.

### 6.2 최종 권장 전략

```
우선순위 1: Objaverse 혼합 학습 (가장 현실적)
────────────────────────────────────────────
- Objaverse pretrained 모델 사용
- Mouse 데이터 10-20% 혼합으로 domain adaptation
- 3D prior 유지하면서 마우스 특성 학습

우선순위 2: MVDiffusion 자기 증류
────────────────────────────────────────────
- 6개 실제 뷰 각각을 입력으로 MVDiffusion 실행
- 생성된 뷰를 pseudo-GT로 활용
- 데이터 양 증가 효과

우선순위 3 (장기): 3D 스캔 데이터 확보
────────────────────────────────────────────
- 마우스 3D 스캔 → 렌더링 → Objaverse 스타일 데이터
- 이것이 근본적 해결책
- 하지만 리소스 소요 큼
```

### 6.3 기대 효과

| 전략 | Mode Collapse | 3D 품질 | 구현 난이도 |
|------|---------------|---------|-------------|
| 현재 (fine-tune만) | 발생 | 낮음 | 완료 |
| Objaverse 혼합 | 개선 | 중간 | 중간 |
| MVDiffusion 증류 | 약간 개선 | 중-낮음 | 낮음 |
| 3D 스캔 데이터 | 해결 | 높음 | 높음 |

---

## 7. 다음 단계 Action Items

1. [ ] Objaverse 80K 데이터셋 접근 확보
2. [ ] MixedDataset 클래스 구현 (Objaverse + Mouse 혼합)
3. [ ] MVDiffusion 자기 증류 스크립트 작성
4. [ ] 혼합 비율 실험 (90:10, 80:20, 70:30)
5. [ ] Validation 메트릭 개선 (단순 PSNR 외에 3D consistency 측정)

---

## 8. 참고 자료

- [GS-LRM Paper](https://arxiv.org/abs/2404.19702)
- [LGM Paper](https://arxiv.org/abs/2402.05054) - Objaverse 80K subset 사용
- [FaceLift GitHub](https://github.com/weijielyu/FaceLift)
- 기존 보고서: `251215_research_stage1_mvdiffusion.md`, `251215_research_stage2_3d_reconstruction.md`
