#!/usr/bin/env python3
"""
LGM Inference from MVDiffusion 4-view Images

This script takes 4 view images (already generated by MVDiffusion) and runs
LGM to produce a single 3D Gaussian reconstruction.

Unlike infer.py which generates 4 views via MVDream, this directly uses
pre-generated views from MVDiffusion.

Usage:
    python scripts/lgm_from_mvdiffusion.py \
        --input_dir outputs/lgm_test/cam_000/lgm_4views \
        --output_dir outputs/lgm_3d \
        --checkpoint LGM/pretrained/model_fp16.safetensors
"""

import os
import sys
import argparse
from pathlib import Path

import numpy as np
import torch
import torch.nn.functional as F
import torchvision.transforms.functional as TF
import imageio
import tqdm
from PIL import Image
from safetensors.torch import load_file

# Add LGM to path
sys.path.insert(0, str(Path(__file__).parent.parent / "LGM"))

from kiui.cam import orbit_camera
from core.options import Options
from core.models import LGM

IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)
IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)


def load_lgm_model(checkpoint_path: str, device: str = "cuda"):
    """Load LGM model with pretrained weights."""
    # Use 'big' config to match pretrained checkpoint
    opt = Options(
        input_size=256,
        up_channels=(1024, 1024, 512, 256, 128),  # one more decoder
        up_attention=(True, True, True, False, False),
        splat_size=128,
        output_size=512,
        batch_size=1,
        num_views=4,  # We use 4 views
    )

    model = LGM(opt)

    if checkpoint_path.endswith('safetensors'):
        ckpt = load_file(checkpoint_path, device='cpu')
    else:
        ckpt = torch.load(checkpoint_path, map_location='cpu')

    model.load_state_dict(ckpt, strict=False)
    print(f"[INFO] Loaded LGM checkpoint from {checkpoint_path}")

    model = model.half().to(device)
    model.eval()

    return model, opt


def load_4views(input_dir: str, input_size: int = 256):
    """
    Load 4 view images and prepare for LGM input.

    Expected files: view_00.png, view_01.png, view_02.png, view_03.png

    Returns:
        torch.Tensor: [4, 3, H, W] normalized images
    """
    input_dir = Path(input_dir)

    images = []
    for i in range(4):
        img_path = input_dir / f"view_{i:02d}.png"
        if not img_path.exists():
            img_path = input_dir / f"view_{i}.png"
        if not img_path.exists():
            raise FileNotFoundError(f"View image not found: {img_path}")

        img = Image.open(img_path).convert('RGB')
        img = img.resize((input_size, input_size), Image.LANCZOS)
        img = np.array(img).astype(np.float32) / 255.0
        images.append(img)

    # Stack: [4, H, W, 3] -> [4, 3, H, W]
    images = np.stack(images, axis=0)
    images = torch.from_numpy(images).permute(0, 3, 1, 2).float()

    # Normalize with ImageNet stats
    images = TF.normalize(images, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)

    return images


def run_lgm_inference(
    model: LGM,
    opt,
    images: torch.Tensor,
    device: str = "cuda"
):
    """
    Run LGM inference on 4-view images.

    Args:
        model: LGM model
        opt: LGM options
        images: [4, 3, H, W] normalized images
        device: cuda device

    Returns:
        gaussians: [1, N, 14] Gaussian parameters
    """
    # Prepare ray embeddings
    rays_embeddings = model.prepare_default_rays(device)  # [4, 6, H, W]

    # Resize images to input size
    images = images.to(device)
    images = F.interpolate(
        images,
        size=(opt.input_size, opt.input_size),
        mode='bilinear',
        align_corners=False
    )

    # Concatenate with ray embeddings: [4, 9, H, W]
    input_tensor = torch.cat([images, rays_embeddings], dim=1)

    # Add batch dimension: [1, 4, 9, H, W]
    input_tensor = input_tensor.unsqueeze(0)

    with torch.no_grad():
        with torch.autocast(device_type='cuda', dtype=torch.float16):
            gaussians = model.forward_gaussians(input_tensor)

    return gaussians


def render_360_video(
    model: LGM,
    opt,
    gaussians: torch.Tensor,
    output_path: str,
    device: str = "cuda",
    num_frames: int = 180,
    fps: int = 30
):
    """Render 360° rotation video of the 3D Gaussian."""
    tan_half_fov = np.tan(0.5 * np.deg2rad(opt.fovy))
    proj_matrix = torch.zeros(4, 4, dtype=torch.float32, device=device)
    proj_matrix[0, 0] = 1 / tan_half_fov
    proj_matrix[1, 1] = 1 / tan_half_fov
    proj_matrix[2, 2] = (opt.zfar + opt.znear) / (opt.zfar - opt.znear)
    proj_matrix[3, 2] = - (opt.zfar * opt.znear) / (opt.zfar - opt.znear)
    proj_matrix[2, 3] = 1

    images = []
    elevation = 0
    azimuth_step = 360 / num_frames

    for i in tqdm.tqdm(range(num_frames), desc="Rendering"):
        azi = i * azimuth_step

        cam_poses = torch.from_numpy(
            orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)
        ).unsqueeze(0).to(device)

        cam_poses[:, :3, 1:3] *= -1  # invert up & forward direction

        cam_view = torch.inverse(cam_poses).transpose(1, 2)
        cam_view_proj = cam_view @ proj_matrix
        cam_pos = - cam_poses[:, :3, 3]

        image = model.gs.render(
            gaussians,
            cam_view.unsqueeze(0),
            cam_view_proj.unsqueeze(0),
            cam_pos.unsqueeze(0),
            scale_modifier=1
        )['image']

        img_np = (image.squeeze(1).permute(0, 2, 3, 1).contiguous().float().cpu().numpy() * 255).astype(np.uint8)
        images.append(img_np)

    images = np.concatenate(images, axis=0)
    imageio.mimwrite(output_path, images, fps=fps)
    print(f"[INFO] Saved video to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="LGM inference from MVDiffusion views")
    parser.add_argument("--input_dir", type=str, required=True,
                        help="Directory containing 4 view images (view_00.png ~ view_03.png)")
    parser.add_argument("--output_dir", type=str, default="outputs/lgm_3d",
                        help="Output directory for 3D results")
    parser.add_argument("--checkpoint", type=str,
                        default="LGM/pretrained/model_fp16.safetensors",
                        help="Path to LGM checkpoint")
    parser.add_argument("--device", type=str, default="cuda",
                        help="Device to use")
    parser.add_argument("--name", type=str, default=None,
                        help="Output name (default: input directory name)")

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Determine output name
    name = args.name or Path(args.input_dir).parent.name

    print("=" * 60)
    print("LGM Inference from MVDiffusion Views")
    print("=" * 60)
    print(f"Input: {args.input_dir}")
    print(f"Output: {output_dir / name}")

    # Load model
    print("\n[Step 1] Loading LGM model...")
    model, opt = load_lgm_model(args.checkpoint, args.device)

    # Load 4 views
    print("\n[Step 2] Loading 4-view images...")
    images = load_4views(args.input_dir, opt.input_size)
    print(f"  Loaded images shape: {images.shape}")

    # Run inference
    print("\n[Step 3] Running LGM inference...")
    gaussians = run_lgm_inference(model, opt, images, args.device)
    print(f"  Generated {gaussians.shape[1]} Gaussians")

    # Save PLY
    print("\n[Step 4] Saving 3D Gaussian...")
    ply_path = output_dir / f"{name}.ply"
    model.gs.save_ply(gaussians, str(ply_path))
    print(f"  Saved: {ply_path}")

    # Render video
    print("\n[Step 5] Rendering 360° video...")
    video_path = output_dir / f"{name}.mp4"
    render_360_video(model, opt, gaussians, str(video_path), args.device)

    print("\n" + "=" * 60)
    print(f"Done! Results saved to {output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()
