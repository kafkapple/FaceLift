# =============================================================================
# Mouse MVDiffusion - Resume from checkpoint-2000
# Continue training from step 2000 to improve quality
# =============================================================================

# Basic model configuration
n_views: 6
img_wh: 512
reference_view_idx: 0
dataset_type: 'mouse'

# Model paths
pretrained_model_name_or_path: 'checkpoints/mvdiffusion/pipeckpts'
pretrained_unet_path: null
revision: null

# Prompt embeddings
prompt_embed_path: "mvdiffusion/data/fixed_prompt_embeds_6view/clr_embeds.pt"

# =============================================================================
# Dataset - use centered data for MVDiffusion
# =============================================================================
train_dataset:
  path: data_mouse_centered/data_mouse_train.txt
  bg_color: 'three_choices'
  augmentation: true
  aug_brightness: [0.9, 1.1]
  aug_contrast: [0.9, 1.1]
  aug_hflip: false

validation_dataset:
  path: data_mouse_centered/data_mouse_val.txt
  bg_color: 'white'

# =============================================================================
# Output Directories
# =============================================================================
checkpoint_prefix: ''
output_dir: 'checkpoints/mvdiffusion/mouse_pixel_based'
val_out_dir: 'checkpoints/mvdiffusion/mouse_pixel_based/val/'

# =============================================================================
# Training Parameters
# =============================================================================
seed: 42
train_batch_size: 2  # Reduced for memory sharing with synthetic data generation
validation_batch_size: 2
max_train_steps: 5000  # Train to step 5000 (from 2000)
gradient_accumulation_steps: 4  # Effective batch = 8
gradient_checkpointing: true
learning_rate: 2e-5  # Lower LR for continued training
step_rules: "1:100000,0.5"
scale_lr: false
lr_scheduler: "piecewise_constant"
lr_warmup_steps: 50
snr_gamma: 5.0
use_8bit_adam: false
allow_tf32: true
use_ema: true
dataloader_num_workers: 4  # Reduced for memory

# Optimizer
adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1.e-2
adam_epsilon: 1.e-08
max_grad_norm: 1.0
prediction_type: null

# =============================================================================
# Logging and Validation
# =============================================================================
vis_dir: vis
logging_dir: logs
mixed_precision: 'fp16'
report_to: 'wandb'
local_rank: -1
checkpointing_steps: 500
checkpoints_total_limit: 5
resume_from_checkpoint: 'checkpoint-2000'  # Resume from step 2000
enable_xformers_memory_efficient_attention: true
validation_steps: 100
validation_sanity_check: false  # Skip sanity check on resume
tracker_project_name: 'mouse_facelift'

# =============================================================================
# Training Specifics
# =============================================================================
trainable_modules: null
use_classifier_free_guidance: true
condition_drop_rate: 0.05
drop_type: 'drop_as_a_whole'
camera_embedding_lr_mult: 1.
scale_input_latents: true

# Pipeline parameters
pipe_kwargs:
  num_views: ${n_views}
validation_guidance_scales: [1., 3.]
validation_grid_nrow: ${n_views}
pipe_validation_kwargs:
  eta: 1.0

# =============================================================================
# UNet Configuration
# =============================================================================
unet_from_pretrained_kwargs:
  unclip: true
  num_views: ${n_views}
  sample_size: 64
  zero_init_conv_in: true
  init_mvattn_with_selfattn: false
  cd_attention_last: false
  cd_attention_mid: false
  multiview_attention: true
  sparse_mv_attention: true
  selfattn_block: custom
  addition_downsample: false

# =============================================================================
# Weights & Biases
# =============================================================================
wandb_exp_name: "mvdiff_resume_2000"
wandb_group: "mvdiffusion"
wandb_job_type: "finetune_resume"
wandb_run_id: "tl22xyuh"  # Resume existing wandb run
