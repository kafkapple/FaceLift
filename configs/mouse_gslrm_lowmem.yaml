# =============================================================================
# Mouse GS-LRM - Low Memory Config (RTX 3060 12GB)
# Optimized for consumer GPUs with limited VRAM
# =============================================================================
# Memory Optimizations:
#   - batch_size: 2 → 1 (saves ~40% VRAM)
#   - grad_accum_steps: 1 → 4 (maintains effective batch size)
#   - amp_dtype: bf16 → fp16 (better on consumer GPUs)
#   - grad_checkpoint_every: 1 (already enabled)
#   - image_size: 512 (unchanged, reduce to 384 if still OOM)
# =============================================================================
profile: false
debug: false

model:
  class_name: gslrm.model.gslrm.GSLRM

  image_tokenizer:
    image_size: 512           # Can reduce to 384 if OOM
    patch_size: 8
    in_channels: 9

  transformer:
    d: 1024
    d_head: 64
    n_layer: 24

  gaussians:
    n_gaussians: 2
    sh_degree: 0
    upsampler:
      upsample_factor: 1

  add_refsrc_marker: false
  hard_pixelalign: true
  use_custom_plucker: true
  clip_xyz: true

training:
  runtime:
    use_tf32: false           # Disable TF32 (not available on RTX 3060)
    use_amp: true
    amp_dtype: "fp16"         # fp16 better than bf16 on consumer GPUs
    torch_compile: false
    grad_accum_steps: 4       # Increased: 1→4 (maintains batch=4 effective)
    grad_clip_norm: 1.0
    grad_checkpoint_every: 1  # Every layer (maximum memory saving)

  dataset:
    dataset_path: "data_mouse_synthetic_test/data_train.txt"
    maximize_view_overlap: false
    num_views: 6
    num_input_views: 1
    target_has_input: true
    normalize_distance_to: 0.0
    remove_alpha: false
    background_color: "white"

  dataloader:
    batch_size_per_gpu: 1     # Reduced: 2→1 (key memory saving)
    num_workers: 2            # Reduced: 4→2 (less CPU memory)
    num_threads: 8            # Reduced: 16→8
    prefetch_factor: 4        # Reduced: 16→4

  losses:
    l2_loss_weight: 1.0
    lpips_loss_weight: 0.5
    perceptual_loss_weight: 0.5
    ssim_loss_weight: 0.2
    pixelalign_loss_weight: 0.0
    masked_pixelalign_loss: true
    pointsdist_loss_weight: 0.0
    warmup_pointsdist: false
    distill_loss_weight: 0.0

  optimizer:
    lr: 0.00002
    beta1: 0.9
    beta2: 0.95
    weight_decay: 0.05
    reset_lr: true
    reset_weight_decay: false
    reset_training_state: false

  schedule:
    num_epochs: 50000
    early_stop_after_epochs: 50000
    max_fwdbwd_passes: 30000
    warmup: 100
    l2_warmup_steps: 100

  checkpointing:
    checkpoint_dir: "checkpoints/gslrm/mouse_lowmem"
    checkpoint_every: 2000
    resume_ckpt: "checkpoints/gslrm/ckpt_0000000000021125.pt"

  logging:
    print_every: 10
    vis_every: 200            # Less frequent vis (memory during eval)
    wandb:
      project: "mouse_facelift"
      group: "gslrm"
      job_type: "finetune_lowmem"
      exp_name: "mouse_gslrm_lowmem"
      log_every: 20
      offline: false

mouse:
  augmentation:
    enabled: true
    brightness_range: [0.9, 1.1]
    contrast_range: [0.9, 1.1]
    hflip: false

validation:
  enabled: true
  dataset_path: "data_mouse_synthetic_test/data_val.txt"
  output_dir: "experiments/validation/mouse_gslrm_lowmem"
  val_every: 1000             # Less frequent validation (memory spikes)

inference:
  enabled: false
  output_dir: "experiments/inference/mouse_gslrm_lowmem"
